OpenAI:
  api_key: 'API_KEY'

# Possible model values
# gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613
# gpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613
# Legacy models
# text-davinci-003, text-davinci-002, code-davinci-002
# See this link for available models
# https://platform.openai.com/docs/models/continuous-model-upgrades
  ai_model: gpt-4
# Local model file to use for transcription.
# Can also be specified using the -m parameter on command line of the application
  local_transcripton_model_file: 'base'
# Language in which ChatGPT should respond.
# List of languages available for openAI is available at
# https://platform.openai.com/docs/guides/speech-to-text/supported-languages
  response_lang: English

# Request timeout for any response requests made to openAI API
  response_request_timeout_seconds: 10

# Summarize request timeout for any response requests made to openAI API
  summarize_request_timeout_seconds: 30

# Temperature parameter for OpenAI chat completion API
  temperature: 0.0

# These are used for single turn responses only. These will go away at some point of time
  default_prompt_preamble: "You are a casual pal, genuinely interested in the conversation at hand. A poor transcription of conversation is given below."
  default_prompt_epilogue: "Please respond, to the conversation in detail. Confidently give a straightforward response to the speaker, even if you don't understand them. Give your response in square brackets. DO NOT ask to repeat, and DO NOT ask for clarification. Aavoid asking questions as much as possible. Just answer the speaker directly."

# The combination of system_prompt, initial_convo is used to create a multi turn prompt message for LLM.
# system_prompt_1, systen_prompt_2 are here as samples of other possible prompts.
# Only the content of system_prompt parameter will be used
  system_prompt: "You are a casual pal, genuinely interested in the conversation at hand. Please respond, in detail, to the conversation. Confidently give a straightforward response to the speaker, even if you don't understand them. Give your response in square brackets. DO NOT ask to repeat, and DO NOT ask for clarification. Just answer the speaker directly."
#  system_prompt: "You are an expert at Basketball and helping others learn about basketball. Please respond, in detail, to the conversation. Confidently give a straightforward response to the speaker, even if you don't understand them. Give your response in square brackets. DO NOT ask to repeat, and DO NOT ask for clarification. Just answer the speaker directly."
#  system_prompt: "You are an expert at Fantasy Football and helping others learn about Fantasy football. Please respond, in detail, to the conversation. Confidently give a straightforward response to the speaker, even if you don't understand them. Give your response in square brackets. DO NOT ask to repeat, and DO NOT ask for clarification. Just answer the speaker directly."
#  system_prompt: â€œYou are an expert Agile Coach and are interviewing for a position. Respond in detail to the conversation. Confidently give a straightforward response to the speaker, even if you don't understand them. Give your response in square brackets. DO NOT ask to repeat, and DO NOT ask for clarification. Just answer the speaker directly."

  summary_prompt: 'Create a summary of the following text'

# When we anticipate to talk about a specific topic, seed the content with some conversation
# Application expects role "You" to have 1 entry
# If the conversation is generic, replace this text with something like this.
#   role: You
#   content: I am V, I want to have a casual friendly conversation
#   role: assistant
#   content: Hello V, That's awesome! Glad to meet you and I am looking forward to our conversation today
  initial_convo:
    first:
      role: "You"
      # content: "I am V, I want to learn about Fantasy Football"
      # content: "I am V, I want to learn about Basketball"
      # content: "Hey assistant, how are you doing today, I want to talk about Agile coaching today."
      content: Hey assistant, how are you doing today, I am in mood of a casual conversation.
    second:
      role: "assistant"
      # content: "Hello, V. That's awesome! What do you want to know about basketball"
      # content: "Hello, V. That's awesome! What do you want to know about Fantasy Football"
      # content: "Hello, V. You are awesome. I am doing very well and looking forward to discussion about Agile coaching."
      content: "Hello, V. You are awesome. I am doing very well and looking forward to some light hearted banter with you."

Deepgram:
  api_key: 'API_KEY'

WhisperCpp:
  # Can also be specified using the -m parameter on command line of the application
  local_transcripton_model_file: 'base'

General:
  log_file: 'logs/Transcribe.log'
  # These two parameters are used together.
  # Save LLM response to file if save_llm_response_to_file is Yes
  save_llm_response_to_file: Yes # Possible values are Yes, No
  llm_response_file: 'logs/response.txt'
# Attempt transcription of the sound file after these number of seconds
  transcript_audio_duration_seconds: 3
# These two parameters are used together.
# Setting clear_transcript_periodically: yes will clear transcript data at a regular interval
# clear_transcript_interval_seconds is applicable when clear_transcript_periodically is set to Yes
  clear_transcript_periodically: No # Possible values are Yes, No
  clear_transcript_interval_seconds: 90
# Determines whether to use API for STT or not. This option is applicable only for online services.
# This option has no affect on offline services.
# This is equivalent to -stt (speech_to_text) argument on command line.
# Command line argument takes precedence over value specified in parameters.yaml
  use_api: False
# Index of microphone device. Value of -1 indicates it is not set.
# This is equivalent to -mi (mic_device_index) argument on command line
# Command line argument takes precedence over value specified in parameters.yaml
  mic_device_index: -1
# Index of speaker device.  Value of -1 indicates it is not set.
# This is equivalent to -si (speaker_device_index) argument on command line
# Command line argument takes precedence over value specified in parameters.yaml
  speaker_device_index: -1
# Disable Microphone
# This is equivalent to -dm (disable_mic) argument on command line
# Command line argument takes precedence over value specified in parameters.yaml
  disable_mic: False
# This is equivalent to -ds (disable_speaker) argument on command line
# Command line argument takes precedence over value specified in parameters.yaml
  disable_speaker: False
  stt: whisper
  continuous_response: True
  # The interval at which to ping the LLM for response
  response_interval: 10
